ğŸ”® Customer Churn Prediction
Can we predict who's about to leave before they actually do? This project says: Yes! ğŸš€
We built a Customer Churn Prediction pipeline that dives into data, battles imbalance ğŸ¥Š, and optimizes metrics that matter most â€” precision ğŸ¯ & recall ğŸ”.

Big thanks to Uneeq Interns ğŸ’¼ for the opportunity to bring this to life!

ğŸ“Š Project Overview
From raw CSVs ğŸ“‚ to actionable insights ğŸ“ˆ â€” this notebook takes you through:

ğŸ§¹ Data cleaning & preprocessing

ğŸ•µï¸ Exploratory Data Analysis (EDA)

âš–ï¸ Handling imbalanced classes with SMOTE & undersampling

ğŸ¤– Training multiple models (LogisticRegression, RandomForestClassifier, XGBClassifier)

ğŸ§® Evaluating with precision, recall, F1-score, ROC AUC

ğŸ“‚ Dataset
/kaggle/input/customer-churn-train-test/customer_churn_dataset-training-master.csv

/kaggle/input/customer-churn-train-test/customer_churn_dataset-testing-master.csv

ğŸ› ï¸ Tech Stack
Languages & Libraries:

from imblearn.over_sampling import SMOTE

from imblearn.under_sampling import RandomUnderSampler

from sklearn.ensemble import RandomForestClassifier

from sklearn.linear_model import LogisticRegression

from sklearn.metrics import (classification_report, confusion_matrix,

from sklearn.metrics import auc

from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV

from sklearn.preprocessing import StandardScaler, LabelEncoder

import matplotlib.pyplot as plt

import numpy as np

import pandas as pd

import seaborn as sns

import shap

import warnings

import xgboost as xgb

ğŸ§  Models Trained
LogisticRegression ğŸ¤–

RandomForestClassifier ğŸ¤–

XGBClassifier ğŸ¤–

ğŸ“ˆ Results
See the notebook for:

Confusion Matrix ğŸŒ€

Classification Report ğŸ“œ

ROC Curve ğŸ“Š

Precision & Recall scores ğŸ“Œ

ğŸ™ Acknowledgements
Special thanks to Uneeq Interns ğŸ’¼ for the mentorship and guidance!
